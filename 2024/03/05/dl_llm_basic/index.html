

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/galaxy.png">
  <link rel="icon" href="/img/galaxy.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Haibin Li">
  <meta name="keywords" content="">
  
    <meta name="description" content="深度学习&amp;LLM基础 1.Attention 1.1 讲讲对Attention的理解 Attention机制是一种在处理时序相关问题的时候常用的技术，主要用于处理序列数据。 核心思想是在处理序列数据时，网络应该更关注输入中的重要部分，而忽略不重要的部分，它通过学习不同部分的权重，将输入的序列中的重要部分显式地加权，从而使得模型可以更好地关注与输出有关的信息。 在序列建模任务中，比如机器翻">
<meta property="og:type" content="article">
<meta property="og:title" content="【深度学习】DeepL｜LLM基础知识">
<meta property="og:url" content="https://lihaibineric.github.io/2024/03/05/dl_llm_basic/index.html">
<meta property="og:site_name" content="LIHAIBIN&#39;S BLOG">
<meta property="og:description" content="深度学习&amp;LLM基础 1.Attention 1.1 讲讲对Attention的理解 Attention机制是一种在处理时序相关问题的时候常用的技术，主要用于处理序列数据。 核心思想是在处理序列数据时，网络应该更关注输入中的重要部分，而忽略不重要的部分，它通过学习不同部分的权重，将输入的序列中的重要部分显式地加权，从而使得模型可以更好地关注与输出有关的信息。 在序列建模任务中，比如机器翻">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240305210627099.png">
<meta property="og:image" content="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240305210748093.png">
<meta property="og:image" content="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240314112600172.png">
<meta property="og:image" content="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240314113451952.png">
<meta property="og:image" content="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240314113638944.png">
<meta property="article:published_time" content="2024-03-05T11:56:50.000Z">
<meta property="article:modified_time" content="2024-03-24T06:36:25.802Z">
<meta property="article:author" content="Haibin Li">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240305210627099.png">
  
  
  
  <title>【深度学习】DeepL｜LLM基础知识 - LIHAIBIN&#39;S BLOG</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"lihaibineric.github.io","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":50,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 36vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>LIHAIBIN&#39;S HOMEPAGE</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" false
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="【深度学习】DeepL｜LLM基础知识"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-03-05 19:56" pubdate>
          March 5, 2024 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          35k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          290 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">【深度学习】DeepL｜LLM基础知识</h1>
            
              <p class="note note-info">
                
                  
                    Last updated on March 24, 2024 pm
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <meta name="referrer" content="no-referrer"/>
<h1 id="深度学习llm基础">深度学习&amp;LLM基础</h1>
<h1 id="attention">1.Attention</h1>
<h2 id="讲讲对attention的理解"><strong>1.1
讲讲对Attention的理解</strong></h2>
<p>Attention机制是一种在处理时序相关问题的时候常用的技术，主要用于处理序列数据。</p>
<p>核心思想是在处理序列数据时，网络应该更关注输入中的重要部分，而忽略不重要的部分，它通过学习不同部分的权重，将输入的序列中的重要部分显式地加权，从而使得模型可以更好地关注与输出有关的信息。</p>
<p>在序列建模任务中，比如机器翻译、文本摘要、语言理解等，输入序列的不同部分可能具有不同的重要性。传统的循环神经网络（RNN）或卷积神经网络（CNN）在处理整个序列时，难以捕捉到序列中不同位置的重要程度，可能导致信息传递不够高效，特别是在处理长序列时表现更明显。</p>
<p>Attention机制的关键是引入一种机制来动态地计算输入序列中各个位置的权重，从而在每个时间步上，对输入序列的不同部分进行加权求和，得到当前时间步的输出。这样就实现了模型对输入中不同部分的关注度的自适应调整。</p>
<h2 id="attention的计算步骤"><strong>1.2
Attention的计算步骤</strong></h2>
<p>具体的计算步骤如下：</p>
<ul>
<li><strong>计算查询（Query）</strong>：查询是当前时间步的输入，用于和序列中其他位置的信息进行比较。</li>
<li><strong>计算键（Key）和值（Value）</strong>：键表示序列中其他位置的信息，值是对应位置的表示。键和值用来和查询进行比较。</li>
<li><strong>计算注意力权重</strong>：通过将查询和键进行内积运算，然后应用softmax函数，得到注意力权重。这些权重表示了在当前时间步，模型应该关注序列中其他位置的重要程度。</li>
<li><strong>加权求和</strong>：根据注意力权重将值进行加权求和，得到当前时间步的输出。</li>
</ul>
<p>在Transformer中，Self-Attention 被称为"Scaled Dot-Product
Attention"，其计算过程如下：</p>
<ol type="1">
<li>对于输入序列中的每个位置，通过计算其与所有其他位置之间的相似度得分（通常通过点积计算）。</li>
<li>对得分进行缩放处理，以防止梯度爆炸。</li>
<li>将得分用softmax函数转换为注意力权重，以便计算每个位置的加权和。</li>
<li>使用注意力权重对输入序列中的所有位置进行加权求和，得到每个位置的自注意输出。</li>
</ol>
<p><span class="math display">\[
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</span></p>
<h2 id="attention机制和传统的seq2seq模型的区别"><strong>1.3
Attention机制和传统的Seq2Seq模型的区别</strong></h2>
<p>Seq2Seq模型是一种基于编码器-解码器结构的模型，主要用于处理序列到序列的任务，例如机器翻译、语音识别等。</p>
<p>传统的Seq2Seq模型只使用编码器来捕捉输入序列的信息，而解码器只从编码器的最后状态中获取信息，并将其用于生成输出序列。</p>
<p>而Attention机制则允许解码器在生成每个输出时，根据输入序列的不同部分给予不同的注意力，从而使得模型更好地关注到输入序列中的重要信息。</p>
<h2 id="self-attention-和-target-attention的区别"><strong>1.4
self-attention 和 target-attention的区别</strong></h2>
<p>self-attention是指在序列数据中，<strong>将当前位置与其他位置之间的关系建模</strong>。它通过计算每个位置与其他所有位置之间的相关性得分，从而为每个位置分配一个权重。这使得模型能够根据输入序列的不同部分的重要性，自适应地选择要关注的信息。</p>
<p>target-attention则是指将<strong>注意力机制应用于目标（或查询）和一组相关对象之间的关系</strong>。它用于将目标与其他相关对象进行比较，并将注意力分配给与目标最相关的对象。这种类型的注意力通常用于任务如机器翻译中的编码-解码模型，其中需要将源语言的信息对齐到目标语言。</p>
<p>因此，<strong>自注意力主要关注序列内部的关系，而目标注意力则关注目标与其他对象之间的关系</strong>。这两种注意力机制在不同的上下文中起着重要的作用，帮助模型有效地处理序列数据和相关任务。</p>
<h2 id="在常规attention中一般有kv那self-attention-可以吗"><strong>1.5
在常规attention中，一般有k=v，那self-attention 可以吗</strong></h2>
<p>self-attention实际只是attention中的一种特殊情况，因此k=v是没有问题的，也即K，V参数矩阵相同。实际上，在Transformer模型中，Self-Attention的典型实现就是k等于v的情况。Transformer中的Self-Attention被称为"Scaled
Dot-Product
Attention"，其中通过将词向量进行线性变换来得到Q、K、V，并且这三者是相等的。</p>
<h2 id="目前主流的attention方法有哪些"><strong>1.6
目前主流的attention方法有哪些？</strong></h2>
<p>讲自己熟悉的就可：</p>
<ul>
<li><strong>Scaled Dot-Product Attention</strong>:
这是Transformer模型中最常用的Attention机制，用于计算查询向量（Q）与键向量（K）之间的相似度得分，然后使用注意力权重对值向量（V）进行加权求和。</li>
<li><strong>Multi-Head Attention</strong>:
这是Transformer中的一个改进，通过同时使用多组独立的注意力头（多个QKV三元组），并在输出时将它们拼接在一起。这样的做法允许模型在<strong>不同的表示空间</strong>上学习不同类型的注意力模式。</li>
<li><strong>Relative Positional Encoding</strong>:
传统的Self-Attention机制在处理序列时并未直接考虑位置信息，而相对位置编码引入了位置信息，使得模型能够更好地处理序列中不同位置之间的关系。</li>
<li><strong>Transformer-XL</strong>:
一种改进的Transformer模型，通过使用循环机制来扩展Self-Attention的上下文窗口，从而处理更长的序列依赖性。</li>
</ul>
<h2 id="self-attention-在计算的过程中如何对padding位做mask"><strong>1.7
self-attention 在计算的过程中，如何对padding位做mask？</strong></h2>
<p>在 Attention 机制中，同样需要忽略 padding
部分的影响，这里以transformer
encoder中的self-attention为例：self-attention中，Q和K在点积之后，需要先经过mask再进行softmax，因此，<strong>对于要屏蔽的部分，mask之后的输出需要为负无穷</strong>，这样softmax之后输出才为0。</p>
<h2 id="深度学习中attention与全连接层的区别何在"><strong>1.8
深度学习中attention与全连接层的区别何在？</strong></h2>
<p>这是个非常有意思的问题，要回答这个问题，我们必须重新定义一下Attention。</p>
<p>Transformer
Paper里重新用QKV定义了Attention。所谓的QKV就是Query，Key，Value。如果我们用这个机制来研究传统的RNN
attention，就会发现这个过程其实是这样的：RNN最后一步的output是Q，这个Q
query了每一个中间步骤的K。Q和K共同产生了Attention Score，最后Attention
Score乘以V加权求和得到context。那如果我们不用Attention，单纯用全连接层呢？很简单，全链接层可没有什么Query和Key的概念，只有一个Value，也就是说给每个V加一个权重再加到一起（如果是Self
Attention，加权这个过程都免了，因为V就直接是从raw
input加权得到的。）</p>
<p><strong>可见Attention和全连接最大的区别就是Query和Key</strong>，而这两者也恰好产生了Attention
Score这个Attention中最核心的机制。<strong>而在Query和Key中，我认为Query又相对更重要，因为Query是一个锚点，Attention
Score便是从过计算与这个锚点的距离算出来的</strong>。任何Attention based
algorithm里都会有Query这个概念，但全连接显然没有。</p>
<p>最后来一个比较形象的比喻吧。如果一个神经网络的任务是从一堆白色小球中找到一个略微发灰的，那么全连接就是在里面随便乱抓然后凭记忆和感觉找，而attention则是左手拿一个白色小球，右手从袋子里一个一个抓出来，两两对比颜色，你左手抓的那个白色小球就是Query。</p>
<h2 id="mha">1.9 MHA</h2>
<p>从多头注意力的结构图中，貌似这个所谓的<strong>多个头就是指多组线性变换层</strong>，其实并不是，只有使用了一组线性变化层，即三个变换张量对Q，K，V分别进行线性变换，<strong>这些变换不会改变原有张量的尺寸</strong>，因此每个变换矩阵都是方阵，得到输出结果后，多头的作用才开始显现，每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量。这就是所谓的多头，将每个头的获得的输入送到注意力机制中,
就形成多头注意力机制.</p>
<p>Multi-head
attention允许模型<strong>共同关注来自不同位置的不同表示子空间的信息</strong>，如果只有一个attention
head，它的平均值会削弱这个信息。</p>
<p><span class="math display">\[
MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O \\
where ~ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
\]</span></p>
<p>其中映射由权重矩阵完成：$ W^Q_i ^{d_ d_k} $, <span
class="math inline">\(W^K_i \in \mathbb{R}^{d_{\text{model}} \times
d_k}\)</span>, <span class="math inline">\(W^V_i \in
\mathbb{R}^{d_{\text{model}} \times d_v}\)</span>和<span
class="math inline">\(W^O_i \in \mathbb{R}^{hd_v \times d_{\text{model}}
}\)</span>。</p>
<figure>
<img
src="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240305210401843.png" srcset="/img/loading.gif" lazyload
alt="MHA示意图" />
<figcaption aria-hidden="true">MHA示意图</figcaption>
</figure>
<p><strong>多头注意力作用</strong></p>
<p>这种结构设计能<strong>让每个注意力机制去优化每个词汇的不同特征部分</strong>，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</p>
<p><strong>为什么要做多头注意力机制呢</strong>？</p>
<ul>
<li>一个 dot product
的注意力里面，没有什么可以学的参数。具体函数就是内积，为了识别不一样的模式，希望有不一样的计算相似度的办法。加性
attention 有一个权重可学，也许能学到一些内容。</li>
<li>multi-head attention 给 h 次机会去学习
不一样的投影的方法，使得在投影进去的度量空间里面能够去匹配不同模式需要的一些相似函数，然后把
h 个 heads 拼接起来，最后再做一次投影。</li>
<li>每一个头 hi 是把 Q,K,V 通过 可以学习的 Wq, Wk, Wv 投影到 dv
上，再通过注意力函数，得到 headi。</li>
</ul>
<h2 id="mqa">1.10 MQA</h2>
<p>MQA（Multi Query Attention）最早是出现在2019年谷歌的一篇论文 《Fast
Transformer Decoding: One Write-Head is All You Need》。</p>
<p>MQA的思想其实比较简单，MQA 与 MHA 不同的是，<strong>MQA
让所有的头之间共享同一份 Key 和 Value 矩阵，每个头正常的只单独保留了一份
Query 参数，从而大大减少 Key 和 Value 矩阵的参数量</strong>。</p>
<p><img src="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240305210627099.png" srcset="/img/loading.gif" lazyload alt="MQA示意图" style="zoom:50%;" /></p>
<p>在 Multi-Query Attention
方法中只会保留一个单独的key-value头，这样<strong>虽然可以提升推理的速度，但是会带来精度上的损失</strong>。《Multi-Head
Attention:Collaborate Instead of Concatenate
》这篇论文的第一个思路是<strong>基于多个 MQA 的 checkpoint 进行
finetuning，来得到了一个质量更高的 MQA 模型</strong>。这个过程也被称为
Uptraining。</p>
<p>具体分为两步：</p>
<ol type="1">
<li>对多个 MQA 的 checkpoint 文件进行融合，融合的方法是: 通过对 key 和
value 的 head 头进行 mean pooling 操作，如下图。</li>
<li>对融合后的模型使用少量数据进行 finetune
训练，重训后的模型大小跟之前一样，但是效果会更好</li>
</ol>
<p><img src="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240305210748093.png" srcset="/img/loading.gif" lazyload alt="多个MQA的finetune" style="zoom: 50%;" /></p>
<h2 id="gqa">1.11 GQA</h2>
<p>Google 在 2023 年发表的一篇 <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.13245.pdf"
title="《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》">《GQA:
Training Generalized Multi-Query Transformer Models from Multi-Head
Checkpoints》</a>的论文</p>
<p>如下图所示</p>
<ul>
<li>在 <strong>MHA（Multi Head Attention）</strong>
中，每个头有自己单独的 key-value 对；</li>
<li>在 <strong>MQA（Multi Query Attention）</strong> 中只会有一组
key-value 对；</li>
<li>在 <strong>GQA（Grouped Query Attention）</strong> 中，会对
attention 进行分组操作，query 被分为 N 组，每个组共享一个 Key 和 Value
矩阵。</li>
</ul>
<figure>
<img
src="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240305210859405.png" srcset="/img/loading.gif" lazyload
alt="MHA,MQA,GQA对比" />
<figcaption aria-hidden="true">MHA,MQA,GQA对比</figcaption>
</figure>
<p>GQA-N 是指具有 N 组的 Grouped Query
Attention。GQA-1具有单个组，因此具有单个Key 和
Value，等效于MQA。而GQA-H具有与头数相等的组，等效于MHA。</p>
<p>在基于 Multi-head 多头结构变为 Grouped-query
分组结构的时候，也是采用跟上图一样的方法，对每一组的 key-value 对进行
mean pool 的操作进行参数融合。<strong>融合后的模型能力更综合，精度比
Multi-query 好，同时速度比 Multi-head 快</strong>。</p>
<h2 id="mhamqagqa对比总结">1.12 MHA&amp;MQA&amp;GQA对比总结</h2>
<p>MHA（Multi-head Attention）是标准的多头注意力机制，h个Query、Key 和
Value 矩阵。</p>
<p>MQA（Multi-Query
Attention）是多查询注意力的一种变体，也是用于自回归解码的一种注意力机制。与MHA不同的是，<strong>MQA
让所有的头之间共享同一份 Key 和 Value 矩阵，每个头只单独保留了一份 Query
参数，从而大大减少 Key 和 Value 矩阵的参数量</strong>。</p>
<p>GQA（Grouped-Query
Attention）是分组查询注意力，<strong>GQA将查询头分成G组，每个组共享一个Key
和 Value 矩阵</strong>。GQA-G是指具有G组的grouped-query
attention。GQA-1具有单个组，因此具有单个Key 和
Value，等效于MQA。而GQA-H具有与头数相等的组，等效于MHA。</p>
<p>GQA介于MHA和MQA之间。GQA 综合 MHA 和 MQA ，既不损失太多性能，又能利用
MQA 的推理加速。不是所有 Q 头共享一组 KV，而是分组一定头数 Q 共享一组
KV，比如上图中就是两组 Q 共享一组 KV。</p>
<h1 id="transformer">2.Transformer</h1>
<h2
id="transformer中multi-head-attention中每个head为什么要进行降维"><strong>2.1
transformer中multi-head
attention中每个head为什么要进行降维？</strong></h2>
<p>在Transformer的Multi-Head
Attention中，对每个head进行降维是<strong>为了增加模型的表达能力和效率。</strong></p>
<p>每个head是独立的注意力机制，它们可以学习不同类型的特征和关系。通过使用多个注意力头，Transformer可以并行地学习多种不同的特征表示，从而增强了模型的表示能力。</p>
<p>然而，在使用多个注意力头的同时，注意力机制的计算复杂度也会增加。原始的Scaled
Dot-Product Attention的计算复杂度为<span
class="math inline">\(O(d^2)\)</span>，其中d是输入向量的维度。如果使用h个注意力头，计算复杂度将增加到<span
class="math inline">\(O(hd^2)\)</span>。这可能会导致Transformer在处理大规模输入时变得非常耗时。</p>
<p>为了缓解计算复杂度的问题，Transformer中在每个head上进行降维。在每个注意力头中，输入向量通过线性变换被映射到一个较低维度的空间。这个降维过程使用两个矩阵：一个是查询（Q）和键（K）的降维矩阵<span
class="math inline">\(W_q\)</span>和<span
class="math inline">\(W_k\)</span>，另一个是值（V）的降维矩阵<span
class="math inline">\(W_v\)</span>。</p>
<p>通过降低每个head的维度，Transformer可以在<strong>保持较高的表达能力的同时，大大减少计算复杂度</strong>。降维后的计算复杂度为<span
class="math inline">\((h\hat d ^ 2)\)</span>，其中<span
class="math inline">\(\hat d\)</span>是降维后的维度。通常情况下，<span
class="math inline">\(\hat
d\)</span>会远小于原始维度d，这样就可以显著提高模型的计算效率。</p>
<h2 id="transformer在哪里做了权重共享为什么可以做权重共享"><strong>2.2
transformer在哪里做了权重共享，为什么可以做权重共享？</strong></h2>
<p>Transformer在Encoder和Decoder中都进行了权重共享。</p>
<p>在Transformer中，Encoder和Decoder是由多层的Self-Attention
Layer和前馈神经网络层交叉堆叠而成。<strong>权重共享是指在这些堆叠的层中，相同位置的层共用相同的参数</strong>。</p>
<p>在Encoder中，所有的自注意力层和前馈神经网络层都共享相同的参数。这意味着每一层的自注意力机制和前馈神经网络都使用相同的权重矩阵来进行计算。这种共享保证了每一层都执行相同的计算过程，使得模型能够更好地捕捉输入序列的不同位置之间的关联性。</p>
<p>在Decoder中，除了和Encoder相同的权重共享方式外，还存在另一种特殊的权重共享：<strong>Decoder的自注意力层和Encoder的自注意力层之间也进行了共享</strong>。这种共享方式被称为"masked
self-attention"，因为在解码过程中，当前位置的注意力不能关注到未来的位置（后续位置），以避免信息泄漏。通过这种共享方式，Decoder可以利用Encoder的表示来理解输入序列并生成输出序列。权重共享的好处是大大减少了模型的参数数量，使得Transformer可以更有效地训练，并且更容易进行推理。此外，共享参数还有助于加快训练速度和提高模型的泛化能力，因为模型可以在不同位置共享并学习通用的特征表示。</p>
<h2 id="transformer的点积模型做缩放的原因是什么">2.3
Transformer的点积模型做缩放的原因是什么？</h2>
<p>使用缩放的原因是为了控制注意力权重的尺度，以避免在计算过程中出现梯度爆炸的问题。</p>
<p>Attention的计算是在内积之后进行softmax，主要涉及的运算是<span
class="math inline">\(e^{q \cdot
k}\)</span>，可以大致认为内积之后、softmax之前的数值在<span
class="math inline">\(-3\sqrt{d}\)</span>到<span
class="math inline">\(3\sqrt{d}\)</span>这个范围内，由于d通常都至少是64，所以<span
class="math inline">\(e^{3\sqrt{d}}\)</span>比较大而 <span
class="math inline">\(e^{-3\sqrt{d}}\)</span>比较小，因此经过softmax之后，Attention的分布非常接近一个one
hot分布了，这带来严重的梯度消失问题，导致训练效果差。（例如y=softmax(x)在|x|较大时进入了饱和区，x继续变化y值也几乎不变，即饱和区梯度消失）</p>
<p>相应地，解决方法就有两个:</p>
<ol type="1">
<li>像NTK参数化那样，在内积之后除以 <span
class="math inline">\(\sqrt{d}\)</span>，使q⋅k的方差变为1，对应<span
class="math inline">\(e^3,e^{−3}\)</span>都不至于过大过小，这样softmax之后也不至于变成one
hot而梯度消失了，这也是常规的Transformer如BERT里边的Self
Attention的做法</li>
<li>另外就是不除以 <span
class="math inline">\(\sqrt{d}\)</span>，但是初始化q,k的全连接层的时候，其初始化方差要多除以一个d，这同样能使得使q⋅k的初始方差变为1，T5采用了这样的做法。</li>
</ol>
<h2 id="transformer和rnn对比">2.4 Transformer和RNN对比</h2>
<p>最简单情况：没有残差连接、没有 layernorm、 attention
单头、没有投影。看和 RNN 区别</p>
<ul>
<li>attention 对输入做一个加权和，加权和 进入 point-wise
MLP。（画了多个红色方块 MLP， 是一个权重相同的 MLP）</li>
<li>point-wise MLP 对 每个输入的点 做计算，得到输出。</li>
<li>attention 作用：把整个序列里面的信息抓取出来，做一次汇聚
aggregation</li>
</ul>
<p>RNN 跟 transformer <strong>异：如何传递序列的信</strong>息</p>
<p>RNN 是把上一个时刻的信息输出传入下一个时候做输入。Transformer
通过一个 attention 层，去全局的拿到整个序列里面信息，再用 MLP
做语义的转换。</p>
<p>RNN 跟 transformer <strong>同：语义空间的转换 + 关注点</strong></p>
<p>用一个线性层 or 一个 MLP 来做语义空间的转换。</p>
<p><strong>关注点</strong>：怎么有效的去使用序列的信息。</p>
<h2 id="transformer结构细节">2.5 Transformer结构细节</h2>
<p><strong>Transformer为何使用多头注意力机制？</strong>（为什么不使用一个头）</p>
<ul>
<li>多头保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。可以类比CNN中同时使用<strong>多个滤波器</strong>的作用，直观上讲，多头的注意力<strong>有助于网络捕捉到更丰富的特征/信息。</strong></li>
</ul>
<p><strong>Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</strong>
（注意和第一个问题的区别）</p>
<ul>
<li>使用Q/K/V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。</li>
<li>同时，由softmax函数的性质决定，实质做的是一个soft版本的arg
max操作，得到的向量接近一个one-hot向量（接近程度根据这组数的数量级有所不同）。如果令Q=K，那么得到的模型大概率会得到一个类似单位矩阵的attention矩阵，<strong>这样self-attention就退化成一个point-wise线性映射</strong>。这样至少是违反了设计的初衷。</li>
</ul>
<p><strong>Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</strong></p>
<ul>
<li>K和Q的点乘是为了得到一个attention score
矩阵，用来对V进行提纯。K和Q使用了不同的W_k,
W_Q来计算，可以理解为是在不同空间上的投影。正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的attention
score矩阵的泛化能力更高。</li>
<li>为了计算更快。矩阵加法在加法这一块的计算量确实简单，但是作为一个整体计算attention的时候相当于一个隐层，整体计算量和点积相似。在效果上来说，从实验分析，两者的效果和dk相关，dk越大，加法的效果越显著。</li>
</ul>
<p><strong>为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根）</strong>，并使用公式推导进行讲解</p>
<ul>
<li>这取决于softmax函数的特性，如果softmax内计算的数数量级太大，会输出近似one-hot编码的形式，导致梯度消失的问题，所以需要scale</li>
<li>那么至于为什么需要用维度开根号，假设向量q，k满足各分量独立同分布，均值为0，方差为1，那么qk点积均值为0，方差为dk，从统计学计算，若果让qk点积的方差控制在1，需要将其除以dk的平方根，是的softmax更加平滑</li>
</ul>
<p><strong>在计算attention
score的时候如何对padding做mask操作？</strong></p>
<ul>
<li>padding位置置为负无穷(一般来说-1000就可以)，再对attention
score进行相加。对于这一点，涉及到batch_size之类的，具体的大家可以看一下实现的源代码，位置在这里：<a
href="https://link.zhihu.com/?target=https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720"
title="https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720">https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720</a></li>
<li>padding位置置为负无穷而不是0，是因为后续在softmax时，<span
class="math inline">\(e^0=1\)</span>，不是0，计算会出现错误；而<span
class="math inline">\(e^{-\infty} = 0\)</span>，所以取负无穷</li>
</ul>
<p><strong>为何在获取输入词向量之后需要对矩阵乘以embedding
size的开方？意义是什么？</strong></p>
<ul>
<li>embedding
matrix的初始化方式是<code>xavier init</code>，这种方式的方差是1/embedding
size，因此乘以embedding size的开方使得embedding
matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</li>
</ul>
<p><strong>简单介绍一下Transformer的位置编码？有什么意义和优缺点？</strong></p>
<ul>
<li>因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的hidden
embedding都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个token的位置信息，transformer使用了固定的positional
encoding来表示token在句子中的绝对位置信息。</li>
</ul>
<p><strong>你还了解哪些关于位置编码的技术，各自的优缺点是什么？</strong>（参考上一题）</p>
<ul>
<li>相对位置编码（RPE）1.在计算attention score和weighted
value时各加入一个可训练的表示相对位置的参数。2.在生成多头注意力时，把对key来说将绝对位置转换为相对query的位置3.复数域函数，已知一个词在某个位置的词向量表示，可以计算出它在任何位置的词向量表示。前两个方法是词向量+位置编码，属于亡羊补牢，复数域是生成词向量的时候即生成对应的位置信息。</li>
</ul>
<p><strong>为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm
在Transformer的位置是哪里？</strong></p>
<ul>
<li>LN：针对每个样本序列进行Norm，没有样本间的依赖。对一个序列的不同特征维度进行Norm</li>
<li>CV使用BN是认为channel维度的信息对cv方面有重要意义，如果对channel维度也归一化会造成不同通道信息一定的损失。而同理nlp领域认为句子长度不一致，并且各个batch的信息没什么关系，因此只考虑句子内信息的归一化，也就是LN。</li>
</ul>
<p><strong>简答讲一下BatchNorm技术，以及它的优缺点。</strong></p>
<ul>
<li>优点：
<ul>
<li>第一个就是可以解决内部协变量偏移，简单来说训练过程中，各层分布不同，增大了学习难度，BN缓解了这个问题。当然后来也有论文证明BN有作用和这个没关系，而是可以使<strong>损失平面更加的平滑</strong>，从而加快的收敛速度。</li>
<li>第二个优点就是缓解了<strong>梯度饱和问题</strong>（如果使用sigmoid激活函数的话），加快收敛。</li>
</ul></li>
<li>缺点：
<ul>
<li>第一个，batch_size较小的时候，效果差。这一点很容易理解。BN的过程，使用
整个batch中样本的均值和方差来模拟全部数据的均值和方差，在batch_size
较小的时候，效果肯定不好。</li>
<li>第二个缺点就是 BN 在RNN中效果比较差。</li>
</ul></li>
</ul>
<p><strong>Encoder端和Decoder端是如何进行交互的？</strong>（在这里可以问一下关于seq2seq的attention知识）</p>
<ul>
<li>Cross Self-Attention，Decoder提供Q，Encoder提供K，V</li>
</ul>
<p><strong>Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？</strong>（为什么需要decoder自注意力需要进行
sequence mask)</p>
<ul>
<li>让输入序列只看到过去的信息，不能让他看到未来的信息</li>
</ul>
<p><strong>引申一个关于bert问题，bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</strong></p>
<ul>
<li>BERT和transformer的目标不一致，bert是语言的预训练模型，需要充分考虑上下文的关系，而transformer主要考虑句子中第i个元素与前i-1个元素的关系。</li>
</ul>
<h1 id="normalization">3 Normalization</h1>
<h2 id="batch-norm">3.1 <strong>Batch Norm</strong></h2>
<p><strong>为什么要进行BN呢？</strong></p>
<ol type="1">
<li>在深度神经网络训练的过程中，通常以输入网络的每一个mini-batch进行训练，这样每个batch具有不同的分布，使模型训练起来特别困难。</li>
<li>Internal Covariate Shift (ICS)
问题：在训练的过程中，激活函数会改变各层数据的分布，随着网络的加深，这种改变（差异）会越来越大，使模型训练起来特别困难，收敛速度很慢，会出现梯度消失的问题。</li>
</ol>
<p><strong>BN的主要思想：</strong>
针对每个神经元，<strong>使数据在进入激活函数之前，沿着通道计算每个batch的均值、方差，‘强迫’数据保持均值为0，方差为1的正态分布，</strong>
避免发生梯度消失。具体来说，就是把第1个样本的第1个通道，加上第2个样本第1个通道
...... 加上第 N 个样本第1个通道，求平均，得到通道 1 的均值（注意是除以
N×H×W 而不是单纯除以 N，最后得到的是一个代表这个 batch
第1个通道平均值的数字，而不是一个 H×W 的矩阵）。求通道 1
的方差也是同理。对所有通道都施加一遍这个操作，就得到了所有通道的均值和方差。</p>
<p><strong>BN的使用位置：</strong>
全连接层或卷积操作之后，激活函数之前。</p>
<p><strong>BN算法过程：</strong></p>
<ul>
<li>沿着通道计算每个batch的均值</li>
<li>沿着通道计算每个batch的方差</li>
<li>做归一化</li>
<li>加入缩放和平移变量<span class="math inline">\(\gamma\)</span>和
<span class="math inline">\(\beta\)</span></li>
</ul>
<p><strong>加入缩放和平移变量的原因是：</strong>保证每一次数据经过归一化后还保留原有学习来的特征，同时又能完成归一化操作，加速训练。这两个参数是用来学习的参数。</p>
<p>encoder-decoder分类<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/642923989">https://zhuanlan.zhihu.com/p/642923989</a></p>
<p><strong>BN的作用：</strong></p>
<ol type="1">
<li>允许较大的学习率；</li>
<li>减弱对初始化的强依赖性</li>
<li>保持隐藏层中数值的均值、方差不变，让数值更稳定，为后面网络提供坚实的基础；</li>
<li>有轻微的正则化作用（相当于给隐藏层加入噪声，类似Dropout）</li>
</ol>
<p><strong>BN存在的问题：</strong></p>
<ol type="1">
<li>每次是在一个batch上计算均值、方差，如果batch
size太小，则计算的均值、方差不足以代表整个数据分布。</li>
<li><strong>batch size太大：</strong>
会超过内存容量；需要跑更多的epoch，导致总训练时间变长；会直接固定梯度下降的方向，导致很难更新。</li>
</ol>
<h2 id="layer-norm">3.2 Layer Norm</h2>
<p>LayerNorm是大模型也是transformer结构中最常用的归一化操作，简而言之，它的作用是
<strong>对特征张量按照某一维度或某几个维度进行0均值，1方差的归一化</strong>
操作，计算公式为：</p>
<p><span class="math display">\[
\mathrm{y}=\frac{\mathrm{x}-\mathrm{E}(\mathrm{x})}{\sqrt{\mathrm{V}
\operatorname{ar}(\mathrm{x})+\epsilon}} * \gamma+\beta
\]</span></p>
<p>这里的 <span class="math inline">\(x\)</span> 可以理解为**
张量中具体某一维度的所有元素**，比如对于 shape 为 (2,2,4) 的张量
input，若指定归一化的操作为第三个维度，则会对第三个维度中的四个张量（2,2,1），各进行上述的一次计算.</p>
<p>详细形式：</p>
<p><span class="math display">\[
a_{i}=\sum_{j=1}^{m} w_{i j} x_{j}, \quad
y_{i}=f\left(a_{i}+b_{i}\right)
\]</span></p>
<p><span class="math display">\[
\bar{a}_{i}=\frac{a_{i}-\mu}{\sigma} g_{i}, \quad
y_{i}=f\left(\bar{a}_{i}+b_{i}\right),
\]</span></p>
<p><span class="math display">\[
\mu=\frac{1}{n} \sum_{i=1}^{n} a_{i}, \quad \sigma=\sqrt{\frac{1}{n}
\sum_{i=1}^{n}\left(a_{i}-\mu\right)^{2}}.
\]</span></p>
<p>这里结合PyTorch的nn.LayerNorm算子来看比较明白</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">nn.LayerNorm(normalized_shape, eps=<span class="hljs-number">1e-05</span>, elementwise_affine=<span class="hljs-literal">True</span>, device=<span class="hljs-literal">None</span>, dtype=<span class="hljs-literal">None</span>)<br><br></code></pre></td></tr></table></figure>
<ul>
<li><code>normalized_shape</code>：归一化的维度，int（最后一维）list（list里面的维度），还是以（2,2,4）为例，如果输入是int，则必须是4，如果是list，则可以是[4],
[2,4], [2,2,4]，即最后一维，倒数两维，和所有维度</li>
<li><code>eps</code>：加在分母方差上的偏置项，防止分母为0</li>
<li><code>elementwise_affine</code>：是否使用可学习的参数 <span
class="math inline">\(\gamma\)</span> 和 <span
class="math inline">\(\beta\)</span>
，前者开始为1，后者为0，设置该变量为True，则二者均可学习随着训练过程而变化</li>
</ul>
<p>Layer Normalization (LN)
的一个优势是不需要批训练，在单条数据内部就能归一化。LN不依赖于batch
size和输入sequence的长度，因此可以用于batch
size为1和RNN中。<strong>LN用于RNN效果比较明显，但是在CNN上，效果不如BN</strong>。</p>
<h2 id="instance-norm">3.3 Instance Norm</h2>
<p>IN针对图像像素做normalization，最初用于图像的风格化迁移。在图像风格化中，生成结果主要依赖于某个图像实例，feature
map 的各个 channel
的均值和方差会影响到最终生成图像的风格。所以对整个batch归一化不适合图像风格化中，因而对H、W做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。</p>
<p>对于，IN 对每个样本的 H、W 维度的数据求均值和标准差，保留 N 、C
维度，也就是说，它只在 channel 内部求均值和标准差，其公式如下：</p>
<p><span class="math display">\[
y_{t i j k}=\frac{x_{t i j k}-\mu_{t i}}{\sqrt{\sigma_{t
i}^{2}+\epsilon}} \quad \mu_{t i}=\frac{1}{H W} \sum_{l=1}^{W}
\sum_{m=1}^{H} x_{t i l m} \quad \sigma_{t i}^{2}=\frac{1}{H W}
\sum_{l=1}^{W} \sum_{m=1}^{H}\left(x_{t i l m}-m u_{t i}\right)^{2}
\]</span></p>
<h2 id="group-norm">3.4 <strong>Group Norm</strong></h2>
<p><strong>GN是为了解决BN对较小的mini-batch size效果差的问题。</strong>
​</p>
<p>GN适用于占用显存比较大的任务，例如图像分割。对这类任务，可能 batch
size 只能是个位数，再大显存就不够用了。而当 batch size 是个位数时，BN
的表现很差，因为没办法通过几个样本的数据量，来近似总体的均值和标准差。GN
也是独立于 batch 的，它是 LN 和 IN 的折中。</p>
<p><strong>具体方法：</strong> GN 计算均值和标准差时，把每一个样本
feature map 的 channel 分成 G 组，每组将有 C/G 个 channel，然后将这些
channel 中的元素求均值和标准差。各组 channel
用其对应的归一化参数独立地归一化。 <span class="math display">\[
\mu_{n g}(x)=\frac{1}{(C / G) H W} \sum_{c=g C / G}^{(g+1) C / G}
\sum_{h=1}^{H} \sum_{w=1}^{W} x_{n c h w}
\]</span></p>
<p><span class="math display">\[
\sigma_{n g}(x)=\sqrt{\frac{1}{(C / G) H W} \sum_{c=g C / G}^{(g+1) C /
G} \sum_{h=1}^{H} \sum_{w=1}^{W}\left(x_{n c h w}-\mu_{n
g}(x)\right)^{2}+\epsilon}
\]</span></p>
<h2 id="几类归一化区别与联系">3.5 几类归一化区别与联系</h2>
<p><strong>Batch Normalization（Batch Norm）</strong>：
<strong>缺点</strong>：在处理序列数据（如文本）时，Batch
Norm可能不会表现得很好，因为序列数据通常长度不一，并且一次训练的Batch中的句子的长度可能会有很大的差异；此外，Batch
Norm对于Batch大小也非常敏感。对于较小的Batch大小，Batch
Norm可能会表现得不好，因为每个Batch的统计特性可能会有较大的波动。</p>
<p><strong>Layer Normalization（Layer Norm）</strong>：
<strong>优点</strong>：Layer
Norm是对每个样本进行归一化，因此它对Batch大小不敏感，这使得它在处理序列数据时表现得更好；另外，Layer
Norm在处理不同长度的序列时也更为灵活。</p>
<p><strong>Instance Normalization（Instance Norm）</strong>：
<strong>优点</strong>：Instance
Norm是对每个样本的每个特征进行归一化，因此它可以捕捉到更多的细节信息。Instance
Norm在某些任务，如风格迁移，中表现得很好，因为在这些任务中，细节信息很重要。
<strong>缺点</strong>：Instance
Norm可能会过度强调细节信息，忽视了更宏观的信息。此外，Instance
Norm的计算成本相比Batch Norm和Layer Norm更高。</p>
<p><strong>Group Normalization（Group Norm）</strong>：
<strong>优点</strong>：Group Norm是Batch Norm和Instance
Norm的折中方案，它在Batch的一个子集（即组）上进行归一化。这使得Group
Norm既可以捕捉到Batch的统计特性，又可以捕捉到样本的细节信息。此外，Group
Norm对Batch大小也不敏感。 <strong>缺点</strong>：Group
Norm的性能取决于组的大小，需要通过实验来确定最优的组大小。此外，Group
Norm的计算成本也比Batch Norm和Layer Norm更高。</p>
<p>将输入的 <strong>feature map shape</strong>
记为**<code>[N, C, H, W]</code>**，其中N表示batch
size，即N个样本；C表示通道数；H、W分别表示特征图的高度、宽度。这几个方法主要的区别就是在：</p>
<ol type="1">
<li>BN是在batch上，对N、H、W做归一化，而保留通道 C
的维度。<strong>BN对较小的batch
size效果不好。BN适用于固定深度的前向神经网络</strong>，如CNN，不适用于RNN；</li>
<li>LN在通道方向上，对C、H、W归一化，主要对RNN效果明显；</li>
<li>IN在图像像素上，对H、W做归一化，用在风格化迁移；</li>
<li>GN将channel分组，然后再做归一化。</li>
</ol>
<figure>
<img
src="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240314112440369.png" srcset="/img/loading.gif" lazyload
alt="几类归一化方式的对比图" />
<figcaption aria-hidden="true">几类归一化方式的对比图</figcaption>
</figure>
<h2 id="rms-norm">3.6 RMS Norm</h2>
<p>与layerNorm相比，RMS
Norm的主要区别在于<strong>去掉了减去均值的部分</strong>，计算公式为：</p>
<p><span class="math display">\[
\bar{a}_{i}=\frac{a_{i}}{\operatorname{RMS}(\mathbf{a})} g_{i}, \quad
where~ \operatorname{RMS}(\mathbf{a})=\sqrt{\frac{1}{n} \sum_{i=1}^{n}
a_{i}^{2}}.
\]</span></p>
<p>RMS中去除了<code>mean</code>的统计值的使用，只使用<code>root mean square(RMS)</code>进行归一化。</p>
<h2 id="prmsnorm介绍">3.7 pRMSNorm介绍</h2>
<p>RMS具有线性特征，所以提出可以用部分数据的RMSNorm来代替全部的计算，pRMSNorm表示使用前p%的数据计算RMS值。k=n*p表示用于RMS计算的元素个数。实测中，使用6.25%的数据量可以收敛</p>
<p><span class="math display">\[
\overline{\operatorname{RMS}}(\mathbf{a})=\sqrt{\frac{1}{k}
\sum_{i=1}^{k} a_{i}^{2}}
\]</span></p>
<h2 id="post-ln-和-pre-ln">3.8 Post-LN 和 Pre-LN</h2>
<p><img src="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240314112600172.png" srcset="/img/loading.gif" lazyload alt="Post-LN和Pre-LN的对比图" style="zoom:50%;" /></p>
<p>左边是原版Transformer的Post-LN，即将LN放在addition之后；右边是改进之后的Pre-LN，即把LN放在FFN和MHA之前。</p>
<p>一般认为，Post-Norm在残差之后做归一化，对参数正则化的效果更强，进而模型的收敛性也会更好；而Pre-Norm有一部分参数直接加在了后面，没有对这部分参数进行正则化，可以在反向时防止梯度爆炸或者梯度消失，大模型的训练难度大，因而使用Pre-Norm较多。</p>
<p>目前比较明确的结论是：<strong>同一设置之下，Pre
Norm结构往往更容易训练，但最终效果通常不如Post Norm</strong>。Pre
Norm更容易训练好理解，因为它的恒等路径更突出，但为什么它效果反而没那么好呢？这个是解释的链接：<a
target="_blank" rel="noopener" href="https://kexue.fm/archives/9009">https://kexue.fm/archives/9009</a></p>
<h1 id="pytorch几类优化器">4 PyTorch几类优化器</h1>
<h2 id="batch-norm-1">4.1 <strong>Batch Norm</strong></h2>
<h1 id="位置编码">5.位置编码</h1>
<h2 id="绝对位置编码">5.1 绝对位置编码</h2>
<p>不同于RNN、CNN等模型，对于Transformer模型来说，位置编码的加入是必不可少的，因为<strong>纯粹的Attention模块是无法捕捉输入顺序的，即无法区分不同位置的Token</strong>。为此我们大体有两个选择：</p>
<ol type="1">
<li>想办法将位置信息融入到输入中，这构成了绝对位置编码的一般做法；</li>
<li>想办法微调一下Attention结构，使得它有能力分辨不同位置的Token，这构成了相对位置编码的一般做法。</li>
</ol>
<p>形式上来看，绝对位置编码是相对简单的一种方案，但即便如此，也不妨碍各路研究人员的奇思妙想，也有不少的变种。一般来说，绝对位置编码会加到输入中：在输入的第<span
class="math inline">\(k\)</span>个向量<span
class="math inline">\(x_k\)</span>中加入位置向量<span
class="math inline">\(p_k\)</span>变为<span
class="math inline">\(x_k+p_k\)</span>，其中<span
class="math inline">\(p_k\)</span>只依赖于位置编号<span
class="math inline">\(k\)</span>。</p>
<h3 id="训练式">5.1.1 训练式</h3>
<p>直接<strong>将位置编码当作可训练参数</strong>，比如最大长度为512，编码维度为768，那么就初始化一个512×768的矩阵作为位置向量，让它随着训练过程更新。</p>
<p>对于这种训练式的绝对位置编码，一般的认为它的缺点是没有<strong>外推性</strong>，即如果预训练最大长度为512的话，那么最多就只能处理长度为512的句子，再长就处理不了了。当然，也可以将超过512的位置向量随机初始化，然后继续微调。但笔者最近的研究表明，通过层次分解的方式，可以使得绝对位置编码能外推到足够长的范围，同时保持还不错的效果，细节请参考笔者之前的博文<a
target="_blank" rel="noopener" href="https://kexue.fm/archives/7947"
title="《层次分解位置编码，让BERT可以处理超长文本》">《层次分解位置编码，让BERT可以处理超长文本》</a>。因此，<strong>其实外推性也不是绝对位置编码的明显缺点</strong>。</p>
<h3 id="三角式">5.1.2 三角式</h3>
<p>三角函数式位置编码，一般也称为Sinusoidal位置编码，是Google的论文<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762"
title="《Attention is All You Need》">《Attention is All You
Need》</a>所提出来的一个显式解：</p>
<p><span class="math display">\[
\left\{\begin{array}{l}\boldsymbol{p}_{k, 2 i}=\sin \left(k / 10000^{2 i
/ d}\right) \\ \boldsymbol{p}_{k, 2 i+1}=\cos \left(k / 10000^{2 i /
d}\right)\end{array}\right.
\]</span></p>
<p>其中<span class="math inline">\(p_{k,2i}\)</span>,<span
class="math inline">\(p_{k,2i+1}\)</span>分别是位置<span
class="math inline">\(k\)</span>的编码向量的第<span
class="math inline">\(2i\)</span>,<span
class="math inline">\(2i+1\)</span>个分量，<span
class="math inline">\(d\)</span>是位置向量的维度。</p>
<p>很明显，三角函数式位置编码的特点是<strong>有显式的生成规律，因此可以期望于它有一定的外推性</strong>。另外一个使用它的理由是：由于<span
class="math inline">\(\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos
\alpha \sin \beta\)</span>以及<span class="math inline">\(\cos
(\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin
\beta\)</span>，这表明位置<span
class="math inline">\(\alpha+\beta\)</span>的向量可以表示成位置<span
class="math inline">\(\alpha\)</span>和位置<span
class="math inline">\(\beta\)</span>的向量组合，这提供了表达相对位置信息的可能性。但很奇怪的是，现在我们很少能看到直接使用这种形式的绝对位置编码的工作，原因不详。</p>
<h3 id="递归式">5.1.3 递归式</h3>
<p>原则上来说，RNN模型不需要位置编码，它在结构上就自带了学习到位置信息的可能性（因为递归就意味着我们可以训练一个“数数”模型），因此，<strong>如果在输入后面先接一层RNN，然后再接Transformer，那么理论上就不需要加位置编码了</strong>。同理，我们也可以用RNN模型来学习一种绝对位置编码，比如从一个向量<span
class="math inline">\(p_0\)</span>出发，通过递归格式<span
class="math inline">\(p_{k+1}=f(p_k)\)</span>来得到各个位置的编码向量。</p>
<p>ICML 2020的论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.09229"
title="《Learning to Encode Position for Transformer with Continuous Dynamical Model》">《Learning
to Encode Position for Transformer with Continuous Dynamical
Model》</a>把这个思想推到了极致，它<strong>提出了用微分方程（ODE）</strong><span
class="math inline">\(dp_t/dt=h(p_t,t)\)</span><strong>的方式来建模位置编码</strong>，该方案称之为FLOATER。显然，FLOATER也属于递归模型，函数<span
class="math inline">\(h(p_t,t)\)</span>可以通过神经网络来建模，因此这种微分方程也称为神经微分方程，关于它的工作最近也逐渐多了起来。</p>
<p>理论上来说，<strong>基于递归模型的位置编码也具有比较好的外推性，同时它也比三角函数式的位置编码有更好的灵活性</strong>（比如容易证明三角函数式的位置编码就是FLOATER的某个特解）。但是很明显，递归形式的位置编码牺牲了一定的并行性，可能会带速度瓶颈。</p>
<h2 id="相对位置编码">5.2 相对位置编码</h2>
<p>相对位置并没有完整建模每个输入的位置信息，而是在<strong>算Attention的时候考虑当前位置与被Attention的位置的相对距离</strong>，由于自然语言一般更依赖于相对位置，所以相对位置编码通常也有着优秀的表现。对于相对位置编码来说，它的灵活性更大，更加体现出了研究人员的“天马行空”。</p>
<h3 id="经典式">5.2.1经典式</h3>
<p>相对位置编码起源于Google的论文<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02155"
title="《Self-Attention with Relative Position Representations》">《Self-Attention
with Relative Position
Representations》</a>，华为开源的NEZHA模型也用到了这种位置编码，后面各种相对位置编码变体基本也是依葫芦画瓢的简单修改。</p>
<p>一般认为，<strong>相对位置编码是由绝对位置编码启发而来</strong>，考虑一般的带绝对位置编码的Attention：</p>
<p><span class="math display">\[
\left\{\begin{aligned} \boldsymbol{q}_{i} &amp;
=\left(\boldsymbol{x}_{i}+\boldsymbol{p}_{i}\right) \boldsymbol{W}_{Q}
\\ \boldsymbol{k}_{j} &amp;
=\left(\boldsymbol{x}_{j}+\boldsymbol{p}_{j}\right) \boldsymbol{W}_{K}
\\ \boldsymbol{v}_{j} &amp;
=\left(\boldsymbol{x}_{j}+\boldsymbol{p}_{j}\right) \boldsymbol{W}_{V}
\\ a_{i, j} &amp; =\operatorname{softmax}\left(\boldsymbol{q}_{i}
\boldsymbol{k}_{j}^{\top}\right) \\ \boldsymbol{o}_{i} &amp; =\sum_{j}
a_{i, j} \boldsymbol{v}_{j}\end{aligned}\right.
\]</span></p>
<p>其中<code>softmax</code>对j那一维归一化，这里的向量都是指行向量。我们初步展开<span
class="math inline">\(q_ik^T_j\)</span>：</p>
<p><span class="math display">\[
\boldsymbol{q}_{i}
\boldsymbol{k}_{j}^{\top}=\left(\boldsymbol{x}_{i}+\boldsymbol{p}_{i}\right)
\boldsymbol{W}_{Q}
\boldsymbol{W}_{K}^{\top}\left(\boldsymbol{x}_{j}+\boldsymbol{p}_{j}\right)^{\top}=\left(\boldsymbol{x}_{i}
\boldsymbol{W}_{Q}+\boldsymbol{p}_{i}
\boldsymbol{W}_{Q}\right)\left(\boldsymbol{W}_{K}^{\top}
\boldsymbol{x}_{j}^{\top}+\boldsymbol{W}_{K}^{\top}
\boldsymbol{p}_{j}^{\top}\right)
\]</span></p>
<p>为了引入相对位置信息，Google把第一项位置去掉，第二项<span
class="math inline">\(p_jW_K\)</span>改为二元位置向量<span
class="math inline">\(R^K_{i,j}\)</span>，变成</p>
<p><span class="math display">\[
a_{i, j}=\operatorname{softmax}\left(\boldsymbol{x}_{i}
\boldsymbol{W}_{Q}\left(\boldsymbol{x}_{j}
\boldsymbol{W}_{K}+\boldsymbol{R}_{i, j}^{K}\right)^{\top}\right)
\]</span></p>
<p>以及<span class="math inline">\(\boldsymbol{o}_{i}=\sum_{j} a_{i, j}
\boldsymbol{v}_{j}=\sum_{j} a_{i, j}\left(\boldsymbol{x}_{j}
\boldsymbol{W}_{V}+\boldsymbol{p}_{j}
\boldsymbol{W}_{V}\right)\)</span>中的中的<span
class="math inline">\(p_jW_V\)</span>换成<span
class="math inline">\(R^V_{i,j}\)</span>：</p>
<p><span class="math display">\[
\boldsymbol{o}_{i}=\sum_{j} a_{i, j}\left(\boldsymbol{x}_{j}
\boldsymbol{W}_{V}+\boldsymbol{R}_{i, j}^{V}\right)
\]</span></p>
<p>所谓相对位置，是将本来依赖于二元坐标<span
class="math inline">\((i,j)\)</span>的向量<span
class="math inline">\(R^K_{i,j}\)</span>,<span
class="math inline">\(R^V_{i,j}\)</span>，改为只依赖于相对距离<span
class="math inline">\(i−j\)</span>，并且通常来说会进行截断，以适应不同任意的距离:</p>
<p><span class="math display">\[
\begin{array}{l}\boldsymbol{R}_{i,
j}^{K}=\boldsymbol{p}_{K}\left[\operatorname{clip}\left(i-j, p_{\min },
p_{\max }\right)\right] \\ \boldsymbol{R}_{i,
j}^{V}=\boldsymbol{p}_{V}\left[\operatorname{clip}\left(i-j, p_{\min },
p_{\max }\right)\right]\end{array}
\]</span></p>
<p>这样一来，只需要有限个位置编码，就可以表达出任意长度的相对位置（因为进行了截断），不管<span
class="math inline">\(p_K\)</span>,<span
class="math inline">\(p_V\)</span>是选择可训练式的还是三角函数式的，都可以达到处理任意长度文本的需求。</p>
<h3 id="xlnet式">5.2.2 XLNET式</h3>
<p>XLNET式位置编码其实源自Transformer-XL的论文<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.02860"
title="《Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context》">《Transformer-XL:
Attentive Language Models Beyond a Fixed-Length
Context》</a>，只不过因为使用了Transformer-XL架构的<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.08237"
title="XLNET">XLNET</a>模型并在一定程度上超过了BERT后，Transformer-XL才算广为人知，因此这种位置编码通常也被冠以XLNET之名。</p>
<p>XLNET式位置编码源于对上述<span
class="math inline">\(q_ik^T_j\)</span>的完全展开：</p>
<p><span class="math display">\[
\boldsymbol{q}_{i} \boldsymbol{k}_{j}^{\top}=\boldsymbol{x}_{i}
\boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top}
\boldsymbol{x}_{j}^{\top}+\boldsymbol{x}_{i} \boldsymbol{W}_{Q}
\boldsymbol{W}_{K}^{\top} \boldsymbol{p}_{j}^{\top}+\boldsymbol{p}_{i}
\boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top}
\boldsymbol{x}_{j}^{\top}+\boldsymbol{p}_{i} \boldsymbol{W}_{Q}
\boldsymbol{W}_{K}^{\top} \boldsymbol{p}_{j}^{\top}
\]</span></p>
<p>Transformer-XL的做法很简单，直接将<span
class="math inline">\(p_j\)</span>替换为相对位置向量<span
class="math inline">\(R_{i−j}\)</span>，至于两个<span
class="math inline">\(p_i\)</span>，则干脆替换为两个可训练的向量<span
class="math inline">\(u,v\)</span>：</p>
<p><span class="math display">\[
\boldsymbol{x}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top}
\boldsymbol{x}_{j}^{\top}+\boldsymbol{x}_{i} \boldsymbol{W}_{Q}
\boldsymbol{W}_{K}^{\top} \boldsymbol{R}_{i-j}^{\top}+u
\boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top}
\boldsymbol{x}_{j}^{\top}+\boldsymbol{v} \boldsymbol{W}_{Q}
\boldsymbol{W}_{K}^{\top} \boldsymbol{R}_{i-j}^{\top}
\]</span></p>
<p>该编码方式中的<span
class="math inline">\(R_{i−j}\)</span>没有像经典模型那样进行截断，而是直接用了Sinusoidal式的生成方案，由于<span
class="math inline">\(R_{i−j}\)</span>的编码空间与<span
class="math inline">\(x_j\)</span>不一定相同，所以<span
class="math inline">\(R_{i−j}\)</span>前面的<span
class="math inline">\(W^T_K\)</span>换了另一个独立的矩阵<span
class="math inline">\(W^T_{K,R}\)</span>，还有<span
class="math inline">\(uW_Q\)</span> 、<span
class="math inline">\(vW_Q\)</span>可以直接合并为单个<span
class="math inline">\(u\)</span> 、<span
class="math inline">\(v\)</span>，所以最终使用的式子是：</p>
<p><span class="math display">\[
\boldsymbol{x}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top}
\boldsymbol{x}_{j}^{\top}+\boldsymbol{x}_{i} \boldsymbol{W}_{Q}
\boldsymbol{W}_{K, R}^{\top} \boldsymbol{R}_{i-j}^{\top}+\boldsymbol{u}
\boldsymbol{W}_{K}^{\top} \boldsymbol{x}_{j}^{\top}+\boldsymbol{v}
\boldsymbol{W}_{K, R}^{\top} \boldsymbol{R}_{i-j}^{\top}
\]</span></p>
<p>此外，<span
class="math inline">\(v_j\)</span>上的位置偏置就直接去掉了，即直接令<span
class="math inline">\(\boldsymbol{o}_{i}=\sum_{j} a_{i, j}
\boldsymbol{x}_{j}
\boldsymbol{W}_{V}\)</span>。似乎从这个工作开始，后面的相对位置编码都只加到Attention矩阵上去，而不加到<span
class="math inline">\(v_j\)</span>上去了。</p>
<h3 id="t5式">5.2.3 T5式</h3>
<p>T5模型出自文章<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.10683"
title="《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》">《Exploring
the Limits of Transfer Learning with a Unified Text-to-Text
Transformer》</a>，里边用到了一种更简单的相对位置编码。思路依然源自<span
class="math inline">\(q_ik^T_j\)</span>展开式，如果非要分析每一项的含义，那么可以分别理解为“输入-输入”、“输入-位置”、“位置-输入”、“位置-位置”四项注意力的组合。如果我们认为输入信息与位置信息应该是独立（解耦）的，那么它们就不应该有过多的交互，所以“输入-位置”、“位置-输入”两项Attention可以删掉，而<span
class="math inline">\(\boldsymbol{p}_{i} \boldsymbol{W}_{Q}
\boldsymbol{W}_{K}^{\top}
\boldsymbol{p}_{j}^{\top}\)</span>实际上只是一个只依赖于<span
class="math inline">\((i,j)\)</span>的标量，我们可以直接将它作为参数训练出来，即简化为：</p>
<p><span class="math display">\[
\boldsymbol{x}_{i} \boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top}
\boldsymbol{x}_{j}^{\top}+\boldsymbol{\beta}_{i, j}
\]</span></p>
<p>说白了，它仅仅是在Attention矩阵的基础上加一个可训练的偏置项而已，而跟XLNET式一样，在<span
class="math inline">\(v_j\)</span>上的位置偏置则直接被去掉了。包含同样的思想的还有微软在ICLR
2021的论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.15595"
title="《Rethinking Positional Encoding in Language Pre-training》">《Rethinking
Positional Encoding in Language
Pre-training》</a>中提出的TUPE位置编码。</p>
<p>比较“别致”的是，不同于常规位置编码对将<span
class="math inline">\(\beta_{i, j}\)</span>视为<span
class="math inline">\(i−j\)</span>的函数并进行截断的做法，T5对相对位置进行了一个“分桶”处理，即相对位置是<span
class="math inline">\(i−j\)</span>的位置实际上对应的是<span
class="math inline">\(f(i−j)\)</span>位置，映射关系如下： </p>
<table>

<thead>
<tr class="header">
<th><span class="math inline">\(i-j\)</span></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(f(i-j)\)</span></td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
<td>8</td>
<td>8</td>
<td>8</td>
<td>8</td>
<td>9</td>
<td>9</td>
<td>9</td>
<td>9</td>
</tr>
<tr class="even">
<td><span class="math inline">\(i-j\)</span></td>
<td>16</td>
<td>17</td>
<td>18</td>
<td>19</td>
<td>20</td>
<td>21</td>
<td>22</td>
<td>23</td>
<td>24</td>
<td>25</td>
<td>26</td>
<td>27</td>
<td>28</td>
<td>29</td>
<td>30</td>
<td>...</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(f(i-j)\)</span></td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>11</td>
<td>11</td>
<td>11</td>
<td>11</td>
<td>11</td>
<td>11</td>
<td>11</td>
<td>11</td>
<td>...</td>
</tr>
</tbody>
</table>
<p>这个设计的思路其实也很直观，就是比较邻近的位置（0～7），需要比较得精细一些，所以给它们都分配一个独立的位置编码，至于稍远的位置（比如8～11），我们不用区分得太清楚，所以它们可以共用一个位置编码，距离越远，共用的范围就可以越大，直到达到指定范围再clip。</p>
<h3 id="deberta式">5.2.4 DeBERTa式</h3>
<p>DeBERTa也是微软搞的，去年6月就发出来了，论文为<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.03654"
title="《DeBERTa: Decoding-enhanced BERT with Disentangled Attention》">《DeBERTa:
Decoding-enhanced BERT with Disentangled
Attention》</a>，最近又小小地火了一把，一是因为它正式中了ICLR
2021，二则是它登上<a target="_blank" rel="noopener" href="https://super.gluebenchmark.com/"
title="SuperGLUE">SuperGLUE</a>的榜首，成绩稍微超过了T5。</p>
<p>其实DeBERTa的主要改进也是在位置编码上，同样还是从<span
class="math inline">\(q_ik^T_j\)</span>展开式出发，T5是干脆去掉了第2、3项，只保留第4项并替换为相对位置编码，而DeBERTa则刚刚相反，它扔掉了第4项，保留第2、3项并且替换为相对位置编码（果然，科研就是枚举所有的排列组合看哪个最优）：</p>
<p><span class="math display">\[
\boldsymbol{q}_{i} \boldsymbol{k}_{j}^{\top}=\boldsymbol{x}_{i}
\boldsymbol{W}_{Q} \boldsymbol{W}_{K}^{\top}
\boldsymbol{x}_{j}^{\top}+\boldsymbol{x}_{i} \boldsymbol{W}_{Q}
\boldsymbol{W}_{K}^{\top} \boldsymbol{R}_{i,
j}^{\top}+\boldsymbol{R}_{j, i} \boldsymbol{W}_{Q}
\boldsymbol{W}_{K}^{\top} \boldsymbol{x}_{j}^{\top}
\]</span></p>
<p>不过，DeBERTa比较有意思的地方，是提供了使用相对位置和绝对位置编码的一个新视角，它指出NLP的大多数任务可能都只需要相对位置信息，但确实有些场景下绝对位置信息更有帮助，于是它将整个模型分为两部分来理解。以Base版的MLM预训练模型为例，它一共有13层，前11层只是用相对位置编码，这部分称为Encoder，后面2层加入绝对位置信息，这部分它称之为Decoder，还弄了个简称EMD（Enhanced
Mask
Decoder）；至于下游任务的微调截断，则是使用前11层的Encoder加上1层的Decoder来进行。</p>
<h2 id="旋转位置编码-rope篇">5.3 旋转位置编码 RoPE篇</h2>
<p>RoPE旋转位置编码是苏神提出来的一种相对位置编码，之前主要用在自研的语言模型roformer上，后续谷歌Palm和meta的LLaMA等都是采用此位置编码，通过复数形式来对于三角式绝对位置编码的改进。有一些同学可能没看懂苏神的公式推导，我这里来帮助大家推理理解下公式。</p>
<p>通过线性attention演算，现在q和k向量中引入绝对位置信息：</p>
<p><span class="math display">\[
\tilde{\boldsymbol{q}}_{m}=\boldsymbol{f}(\boldsymbol{q}, m), \quad
\tilde{\boldsymbol{k}}_{n}=\boldsymbol{f}(\boldsymbol{k}, n)
\]</span></p>
<p>但是需要实现相对位置编码的话，需要显式融入相对。attention运算中q和k会进行内积，所以考虑在进行向量内积时考虑融入相对位置。所以假设成立恒等式：</p>
<p><span class="math display">\[
\langle\boldsymbol{f}(\boldsymbol{q}, m), \boldsymbol{f}(\boldsymbol{k},
n)\rangle=g(\boldsymbol{q}, \boldsymbol{k}, m-n)
\]</span></p>
<p>其中<code>m-n</code>包含着token之间的相对位置信息。</p>
<p>给上述恒等式计算设置初始条件，例如<span
class="math inline">\(f(q,0)=q\)</span>，<span
class="math inline">\(f(k,0)=k\)</span>。</p>
<p>求解过程使用复数方式求解</p>
<p>将内积使用复数形式表示：</p>
<p><span class="math display">\[
\langle\boldsymbol{q},
\boldsymbol{k}\rangle=\operatorname{Re}\left[\boldsymbol{q}
\boldsymbol{k}^{*}\right]
\]</span></p>
<p>转化上面内积公式可得：</p>
<p><span class="math display">\[
\operatorname{Re}\left[\boldsymbol{f}(\boldsymbol{q}, m)
\boldsymbol{f}^{*}(\boldsymbol{k}, n)\right]=g(\boldsymbol{q},
\boldsymbol{k}, m-n)
\]</span></p>
<p>假设等式两边都存在复数形式，则有下式：</p>
<p><span class="math display">\[
\boldsymbol{f}(\boldsymbol{q}, m) \boldsymbol{f}^{*}(\boldsymbol{k},
n)=\boldsymbol{g}(\boldsymbol{q}, \boldsymbol{k}, m-n)
\]</span></p>
<p>将两边公式皆用复数指数形式表示：</p>
<p>存在<span class="math inline">\(r e^{\theta \mathrm{j}}=r \cos
\theta+r \sin \theta \mathrm{j}\)</span>，即任意复数<span
class="math inline">\(z\)</span>可以表示为<span
class="math inline">\(\boldsymbol{z}=r e^{\theta
\mathrm{j}}\)</span>，其中<span
class="math inline">\(r\)</span>为复数的模，<span
class="math inline">\(\theta\)</span>为幅角。</p>
<p><span class="math display">\[
\begin{aligned} \boldsymbol{f}(\boldsymbol{q}, m) &amp;
=R_{f}(\boldsymbol{q}, m) e^{\mathrm{i} \Theta_{f}(\boldsymbol{q}, m)}
\\ \boldsymbol{f}(\boldsymbol{k}, n) &amp; =R_{f}(\boldsymbol{k}, n)
e^{\mathrm{i} \Theta_{f}(\boldsymbol{k}, n)} \\
\boldsymbol{g}(\boldsymbol{q}, \boldsymbol{k}, m-n) &amp;
=R_{g}(\boldsymbol{q}, \boldsymbol{k}, m-n) e^{\mathrm{i}
\Theta_{g}(\boldsymbol{q}, \boldsymbol{k}, m-n)}\end{aligned}
\]</span></p>
<p>由于带入上面方程中<span
class="math inline">\(f(k,n)\)</span>带*是共轭复数，所以指数形式应该是<span
class="math inline">\(e^{-x}\)</span>形式，带入上式公式可得方程组：</p>
<p><span class="math display">\[
\begin{aligned} R_{f}(\boldsymbol{q}, m) R_{f}(\boldsymbol{k}, n) &amp;
=R_{g}(\boldsymbol{q}, \boldsymbol{k}, m-n) \\
\Theta_{f}(\boldsymbol{q}, m)-\Theta_{f}(\boldsymbol{k}, n) &amp;
=\Theta_{g}(\boldsymbol{q}, \boldsymbol{k}, m-n)\end{aligned}
\]</span></p>
<p>第一个方程带入条件<span
class="math inline">\(m=n\)</span>化简可得：</p>
<p><span class="math display">\[
R_{f}(\boldsymbol{q}, m) R_{f}(\boldsymbol{k}, m)=R_{g}(\boldsymbol{q},
\boldsymbol{k}, 0)=R_{f}(\boldsymbol{q}, 0) R_{f}(\boldsymbol{k},
0)=\|\boldsymbol{q}\|\|\boldsymbol{k}\|
\]</span></p>
<p><span class="math display">\[
R_{f}(\boldsymbol{q}, m)=\|\boldsymbol{q}\|, R_{f}(\boldsymbol{k},
m)=\|\boldsymbol{k}\|
\]</span></p>
<p>从上式可以看出来复数<span
class="math inline">\(f(q,m)\)</span>和<span
class="math inline">\(f(k,m)\)</span>与<span
class="math inline">\(m\)</span>取值关系不大。</p>
<p>第二个方程带入<span class="math inline">\(m=n\)</span>化简可得：</p>
<p><span class="math display">\[
\Theta_{f}(\boldsymbol{q}, m)-\Theta_{f}(\boldsymbol{k},
m)=\Theta_{g}(\boldsymbol{q}, \boldsymbol{k},
0)=\Theta_{f}(\boldsymbol{q}, 0)-\Theta_{f}(\boldsymbol{k},
0)=\Theta(\boldsymbol{q})-\Theta(\boldsymbol{k})
\]</span></p>
<p>上式公式变量两边挪动下得到：</p>
<p><span class="math display">\[
\Theta_{f}(\boldsymbol{q}, m)-\Theta_{f}(\boldsymbol{k},
m)=\Theta_{g}(\boldsymbol{q}, \boldsymbol{k},
0)=\Theta_{f}(\boldsymbol{q}, 0)-\Theta_{f}(\boldsymbol{k},
0)=\Theta(\boldsymbol{q})-\Theta(\boldsymbol{k})
\]</span></p>
<p>其中上式结果相当于m是自变量，结果是与m相关的值，假设为 <span
class="math inline">\(\varphi(m)\)</span>，即<span
class="math inline">\(\Theta_{f}(\boldsymbol{q},
m)=\Theta(\boldsymbol{q})+\varphi(m)\)</span></p>
<p><code>n</code>假设为<code>m</code>的前一个token，则可得<code>n=m-1</code>，带入上上个式子可得：</p>
<p><span class="math display">\[
\varphi(m)-\varphi(m-1)=\Theta_{g}(\boldsymbol{q}, \boldsymbol{k},
1)+\Theta(\boldsymbol{k})-\Theta(\boldsymbol{q})
\]</span></p>
<p>即 <span
class="math inline">\(\varphi(m)\)</span>是等差数列，假设等式右边为
<span class="math inline">\(\theta\)</span>
，则<code>m</code>和<code>m-1</code>位置的公差就是为<span
class="math inline">\(\theta\)</span>，可推得 <span
class="math inline">\(\varphi(m)=m \theta\)</span>。</p>
<p>得到二维情况下用复数表示的RoPE：</p>
<p><span class="math display">\[
\boldsymbol{f}(\boldsymbol{q}, m)=R_{f}(\boldsymbol{q}, m) e^{\mathrm{i}
\Theta_{f}(\boldsymbol{q}, m)}=\|q\|
e^{\mathrm{i}(\Theta(\boldsymbol{q})+m \theta)}=\boldsymbol{q}
e^{\mathrm{i} m \theta}
\]</span></p>
<p>矩阵形式是：</p>
<p><span class="math display">\[
\boldsymbol{f}(\boldsymbol{q}, m)=\left(\begin{array}{cc}\cos m \theta
&amp; -\sin m \theta \\ \sin m \theta &amp; \cos m
\theta\end{array}\right)\left(\begin{array}{l}q_{0} \\
q_{1}\end{array}\right)
\]</span></p>
<p>公式最后还会采用三角式一样的远程衰减，来增加周期性函数外推位置差异性。</p>
<p><span class="math display">\[
\left(\boldsymbol{W}_{m}
\boldsymbol{q}\right)^{\top}\left(\boldsymbol{W}_{n}
\boldsymbol{k}\right)=\operatorname{Re}\left[\sum_{i=0}^{d / 2-1}
\boldsymbol{q}_{[2 i: 2 i+1]} \boldsymbol{k}_{[2 i: 2 i+1]}^{*}
e^{\mathrm{i}(m-n) \theta_{i}}\right]
\]</span></p>
<h2 id="几种位置编码方式总结">5.4 几种位置编码方式总结</h2>
<h3 id="绝对位置编码-1"><strong>5.4.1 绝对位置编码</strong></h3>
<ul>
<li>最原始的正余弦位置编码（即sinusoidal位置编码）是一种绝对位置编码，但从其原理中的正余弦的和差化积公式来看，引入的其实也是相对位置编码。</li>
<li>优势： 实现简单，可预先计算好，不用参与训练，速度快。</li>
<li>劣势：
没有外推性，即如果预训练最大长度为512的话，那么最多就只能处理长度为512的句子，再长就处理不了了。当然，也可以将超过512的位置向量随机初始化，然后继续微调。</li>
</ul>
<h3 id="相对位置编码-1"><strong>5.4.2 相对位置编码</strong></h3>
<ul>
<li>经典相对位置编码RPR式的讲解可看我的博客：相对位置编码之RPR式：《Self-Attention
with Relative Position Representations》论文笔记 【在k,
v中注入相对位置信息】</li>
<li>优势：
直接地体现了相对位置信号，效果更好。具有外推性，处理长文本能力更强。</li>
</ul>
<h3 id="rope"><strong>5.4.3 RoPE</strong></h3>
<ul>
<li>RoPE通过绝对位置编码的方式实现相对位置编码，综合了绝对位置编码和相对位置编码的优点。</li>
<li>主要就是<strong>对attention中的q,
k向量注入了绝对位置信息，然后用更新的q,k向量做attention中的内积就会引入相对位置信息了</strong>。</li>
</ul>
<h1 id="激活函数">6 激活函数</h1>
<h2 id="介绍一下-ffn-块-计算公式">6.1 介绍一下 FFN 块 计算公式？</h2>
<p>FFN（Feed-Forward
Network）块是Transformer模型中的一个重要组成部分，接受自注意力子层的输出作为输入，并通过一个带有
Relu
激活函数的两层全连接网络对输入进行更加复杂的非线性变换。实验证明，这一非线性变换会对模型最终的性能产生十分
重要的影响。</p>
<p>FFN由两个全连接层（即前馈神经网络）和一个激活函数组成。下面是FFN块的计算公式：</p>
<p><span class="math display">\[
\operatorname{FFN}(\boldsymbol{x})=\operatorname{Relu}\left(\boldsymbol{x}
\boldsymbol{W}_{1}+\boldsymbol{b}_{1}\right)
\boldsymbol{W}_{2}+\boldsymbol{b}_{2}
\]</span></p>
<p>假设输入是一个向量 <span
class="math inline">\(x\)</span>，FFN块的计算过程如下：</p>
<ol type="1">
<li>第一层全连接层（线性变换）：<span class="math inline">\(z = xW1 +
b1\)</span> 其中，W1 是第一层全连接层的权重矩阵，b1 是偏置向量。</li>
<li>激活函数：<span class="math inline">\(a = g(z)\)</span> 其中，g()
是激活函数，常用的激活函数有ReLU（Rectified Linear Unit）等。</li>
<li>第二层全连接层（线性变换）：<span class="math inline">\(y = aW2 +
b2\)</span> 其中，W2 是第二层全连接层的权重矩阵，b2 是偏置向量。</li>
</ol>
<p>增大前馈子层隐状态的维度有利于提
升最终翻译结果的质量，因此，前馈子层隐状态的维度一般比自注意力子层要大。</p>
<p>需要注意的是，上述公式中的 W1、b1、W2、b2
是FFN块的可学习参数，它们会通过训练过程进行学习和更新。</p>
<h2 id="介绍一下-gelu-计算公式">6.2 介绍一下 GeLU 计算公式？</h2>
<p>GeLU（Gaussian Error Linear
Unit）是一种激活函数，常用于神经网络中的非线性变换。它在Transformer模型中广泛应用于FFN（Feed-Forward
Network）块。下面是GeLU的计算公式：</p>
<p>假设输入是一个标量 x，GeLU的计算公式如下：</p>
<p><span class="math display">\[
GeLU(x) = 0.5 \times x \times (1 + tanh(\sqrt{\frac{2}{\pi}} \times (x +
0.044715 \times x^3)))
\]</span></p>
<p>其中，<code>tanh()</code>是双曲正切函数，<code>sqrt()</code>
是平方根函数，$ $是圆周率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">GELU</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * x * (<span class="hljs-number">1</span> + np.tanh(np.sqrt(<span class="hljs-number">2</span> / np.pi) * (x + <span class="hljs-number">0.044715</span> * np.power(x, <span class="hljs-number">3</span>))))<br></code></pre></td></tr></table></figure>
<figure>
<img
src="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240314113922791.png" srcset="/img/loading.gif" lazyload
alt="几类激活函数的表示" />
<figcaption aria-hidden="true">几类激活函数的表示</figcaption>
</figure>
<p>相对于 Sigmoid 和 Tanh 激活函数，ReLU 和 GeLU
更为准确和高效，因为它们在神经网络中的梯度消失问题上表现更好。而 ReLU 和
GeLU
几乎没有梯度消失的现象，可以更好地支持深层神经网络的训练和优化。</p>
<p>而 <strong>ReLU 和 GeLU 的区别在于形状和计算效率</strong>。ReLU
是一个非常简单的函数，仅仅是输入为负数时返回0，而输入为正数时返回自身，从而仅包含了一次分段线性变换。但是，<strong>ReLU
函数存在一个问题，就是在输入为负数时，输出恒为0，这个问题可能会导致神经元死亡，从而降低模型的表达能力</strong>。GeLU
函数则是一个连续的 S 形曲线，介于 Sigmoid 和 ReLU 之间，形状比 ReLU
更为平滑，可以在一定程度上缓解神经元死亡的问题。不过，由于 GeLU
函数中包含了指数运算等复杂计算，所以在实际应用中通常比 ReLU 慢。</p>
<p>总之，ReLU 和 GeLU
都是常用的激活函数，它们各有优缺点，并适用于不同类型的神经网络和机器学习问题。一般来说，ReLU
更适合使用在卷积神经网络（CNN）中，而 GeLU
更适用于全连接网络（FNN）。</p>
<h2 id="介绍一下-swish-计算公式">6.3 介绍一下 Swish 计算公式？</h2>
<p>Swish是一种激活函数，它在深度学习中常用于神经网</p>
<p>络的非线性变换。Swish函数的计算公式如下： <span
class="math display">\[
Swish(x) = x \times sigmoid(\beta * x)
\]</span></p>
<p>其中，<span class="math inline">\(sigmoid()\)</span>
是Sigmoid函数，<span class="math inline">\(x\)</span> 是输入，<span
class="math inline">\(\beta\)</span> 是一个可调节的超参数。</p>
<figure>
<img
src="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240314114035744.png" srcset="/img/loading.gif" lazyload
alt="Swish图像随参数变化的趋势" />
<figcaption aria-hidden="true">Swish图像随参数变化的趋势</figcaption>
</figure>
<p>Swish函数的特点是在接近零的区域表现得类似于线性函数，而在远离零的区域则表现出非线性的特性。相比于其他常用的激活函数（如ReLU、tanh等），Swish函数在某些情况下能够提供更好的性能和更快的收敛速度。</p>
<p>Swish函数的设计灵感来自于自动搜索算法，它通过引入一个可调节的超参数来增加非线性程度。当beta为0时，Swish函数退化为线性函数；当beta趋近于无穷大时，Swish函数趋近于ReLU函数。</p>
<p>需要注意的是，Swish函数相对于其他激活函数来说计算开销较大，因为它需要进行Sigmoid运算。因此，在实际应用中，也可以根据具体情况选择其他的激活函数来代替Swish函数。</p>
<h2 id="介绍一下使用-glu-线性门控单元的-ffn-块-计算公式">6.4
介绍一下使用 GLU 线性门控单元的 FFN 块 计算公式？</h2>
<p>使用GLU（Gated Linear Unit）线性门控单元的FFN（Feed-Forward
Network）块是Transformer模型中常用的结构之一。它通过引入门控机制来增强模型的非线性能力。下面是使用GLU线性门控单元的FFN块的计算公式：</p>
<p>假设输入是一个向量 x，GLU线性门控单元的计算公式如下：</p>
<p><span class="math display">\[
GLU(x) = x * sigmoid(W_1 * x)
\]</span></p>
<p>其中，<span class="math inline">\(sigmoid()\)</span>
是Sigmoid函数，<span class="math inline">\(W_1\)</span>
是一个可学习的权重矩阵。</p>
<p>在公式中，首先将输入向量 x 通过一个全连接层（线性变换）得到一个与 x
维度相同的向量，然后将该向量通过Sigmoid函数进行激活。这个Sigmoid函数的输出称为门控向量，用来控制输入向量
x 的元素是否被激活。最后，将门控向量与输入向量 x
逐元素相乘，得到最终的输出向量。</p>
<p>GLU线性门控单元的特点是能够对输入向量进行选择性地激活，从而增强模型的表达能力。它在Transformer模型的编码器和解码器中广泛应用，用于对输入向量进行非线性变换和特征提取。</p>
<p>需要注意的是，GLU线性门控单元的计算复杂度较高，可能会增加模型的计算开销。因此，在实际应用中，也可以根据具体情况选择其他的非线性变换方式来代替GLU线性门控单元。</p>
<h2 id="介绍一下-使用-gelu-的-glu-块-计算公式">6.5 介绍一下 使用 GeLU 的
GLU 块 计算公式？</h2>
<p>使用GeLU作为激活函数的GLU块的计算公式如下：</p>
<p><span class="math display">\[
GLU(x) = x * GeLU(W_1 * x)
\]</span></p>
<p>其中，<code>GeLU()</code>是Gaussian Error Linear
Unit的激活函数，<code>W_1</code>是一个可学习的权重矩阵。</p>
<p>在公式中，首先将输入向量 x 通过一个全连接层（线性变换）得到一个与 x
维度相同的向量，然后将该向量作为输入传递给GeLU激活函数进行非线性变换。最后，将GeLU激活函数的输出与输入向量
x 逐元素相乘，得到最终的输出向量。</p>
<p>GeLU激活函数的计算公式如下：</p>
<p><span class="math display">\[
GeLU(x) = 0.5 \times x \times (1 + tanh(\sqrt{\frac{2}{\pi}} \times (x +
0.044715 \times x^3)))
\]</span></p>
<p>其中，<code>tanh()</code>是双曲正切函数，<code>sqrt()</code>
是平方根函数，$ $是圆周率。</p>
<p>在公式中，GeLU函数首先对输入向量 x
进行一个非线性变换，然后通过一系列的数学运算得到最终的输出值。</p>
<p>使用GeLU作为GLU块的激活函数可以增强模型的非线性能力，并在某些情况下提供更好的性能和更快的收敛速度。这种结构常用于Transformer模型中的编码器和解码器，用于对输入向量进行非线性变换和特征提取。</p>
<p>需要注意的是，GLU块和GeLU激活函数是两个不同的概念，它们在计算公式和应用场景上有所区别。在实际应用中，可以根据具体情况选择合适的激活函数来代替GeLU或GLU。</p>
<h2 id="介绍一下-使用-swish-的-glu-块-计算公式">6.6 介绍一下 使用 Swish
的 GLU 块 计算公式？</h2>
<p>使用Swish作为激活函数的GLU块的计算公式如下：</p>
<p><span class="math display">\[
GLU(x) = x * sigmoid(W_1 * x)
\]</span></p>
<p>其中，<span class="math inline">\(sigmoid()\)</span>
是Sigmoid函数，<span class="math inline">\(W_1\)</span>
是一个可学习的权重矩阵。</p>
<p>在公式中，首先将输入向量 x 通过一个全连接层（线性变换）得到一个与 x
维度相同的向量，然后将该向量通过Sigmoid函数进行激活。这个Sigmoid函数的输出称为门控向量，用来控制输入向量
x 的元素是否被激活。最后，将门控向量与输入向量 x
逐元素相乘，得到最终的输出向量。</p>
<p>Swish激活函数的计算公式如下：</p>
<p><span class="math display">\[
Swish(x) = x \times sigmoid(\beta * x)
\]</span></p>
<p>其中，<span class="math inline">\(sigmoid()\)</span>
是Sigmoid函数，<span class="math inline">\(x\)</span> 是输入，<span
class="math inline">\(\beta\)</span> 是一个可调节的超参数。</p>
<p>在公式中，Swish函数首先对输入向量 x
进行一个非线性变换，然后通过Sigmoid函数进行激活，并将该激活结果与输入向量
x 逐元素相乘，得到最终的输出值。</p>
<p>使用Swish作为GLU块的激活函数可以增强模型的非线性能力，并在某些情况下提供更好的性能和更快的收敛速度。GLU块常用于Transformer模型中的编码器和解码器，用于对输入向量进行非线性变换和特征提取。</p>
<p>需要注意的是，GLU块和Swish激活函数是两个不同的概念，它们在计算公式和应用场景上有所区别。在实际应用中，可以根据具体情况选择合适的激活函数来代替Swish或GLU。</p>
<h1 id="tokenize相关">7 Tokenize相关</h1>
<h2 id="总览">7.1 总览</h2>
<table>

<thead>
<tr class="header">
<th>分词方法</th>
<th>特点</th>
<th>被提出的时间</th>
<th>典型模型</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BPE</td>
<td>采用合并规则，可以适应未知词</td>
<td>2016年</td>
<td>GPT-2、RoBERTa</td>
</tr>
<tr class="even">
<td>WordPiece</td>
<td>采用逐步拆分的方法，可以适应未知词</td>
<td>2016年</td>
<td>BERT</td>
</tr>
<tr class="odd">
<td>Unigram LM</td>
<td>采用无序语言模型，训练速度快</td>
<td>2018年</td>
<td>XLM</td>
</tr>
<tr class="even">
<td>SentencePiece</td>
<td>采用汉字、字符和子词三种分词方式，支持多语言</td>
<td>2018年</td>
<td>T5、ALBERT</td>
</tr>
</tbody>
</table>
<h3 id="背景与基础">背景与基础</h3>
<p>在使用GPT BERT模型输入词语常常会先进行tokenize
，tokenize的目标是把输入的文本流，<strong>切分成一个个子串，每个子串相对有完整的语义</strong>，便于学习embedding表达和后续模型的使用。</p>
<p>tokenize有三种粒度：<strong>word/subword/char</strong></p>
<ul>
<li><strong>word/词</strong>，词，是最自然的语言单元。对于英文等自然语言来说，存在着天然的分隔符，如空格或一些标点符号等，对词的切分相对容易。但是对于一些东亚文字包括中文来说，就需要某种分词算法才行。顺便说一下，Tokenizers库中，基于规则切分部分，<strong>采用了spaCy和Moses两个库</strong>。如果基于词来做词汇表，由于长尾现象的存在，<strong>这个词汇表可能会超大</strong>。像Transformer
XL库就用到了一个<strong>26.7万</strong>个单词的词汇表。这需要极大的embedding
matrix才能存得下。embedding matrix是用于查找取用token的embedding
vector的。这对于内存或者显存都是极大的挑战。常规的词汇表，<strong>一般大小不超过5万</strong>。</li>
<li><strong>char/字符</strong>，即最基本的字符，如英语中的'a','b','c'或中文中的'你'，'我'，'他'等。而一般来讲，字符的数量是<strong>少量有限</strong>的。这样做的问题是，由于字符数量太小，我们在为每个字符学习嵌入向量的时候，每个向量就容纳了太多的语义在内，学习起来非常困难。</li>
<li><strong>subword/子词级</strong>，它介于字符和单词之间。比如说'Transformers'可能会被分成'Transform'和'ers'两个部分。这个方案<strong>平衡了词汇量和语义独立性</strong>，是相对较优的方案。它的处理原则是，<strong>常用词应该保持原状，生僻词应该拆分成子词以共享token压缩空间</strong>。</li>
</ul>
<h2 id="常用的tokenize算法">7.2 常用的tokenize算法</h2>
<p>最常用的三种tokenize算法：BPE（Byte-Pair
Encoding)，WordPiece和SentencePiece</p>
<h3 id="bpebyte-pair-encoding">7.2.1 BPE（Byte-Pair Encoding)</h3>
<p>BPE，即字节对编码。其核心思想在于将<strong>最常出现的子词对合并，直到词汇表达到预定的大小时停止</strong>。</p>
<p>BPE是一种基于数据压缩算法的分词方法。它通过不断地合并出现频率最高的字符或者字符组合，来构建一个词表。具体来说，BPE的运算过程如下：</p>
<ol type="1">
<li>将所有单词按照字符分解为字母序列。例如：“hello”会被分解为["h","e","l","l","o"]。</li>
<li>统计每个字母序列出现的频率，将频率最高的序列合并为一个新序列。</li>
<li>重复第二步，直到达到预定的词表大小或者无法再合并。</li>
</ol>
<p>词表大小通常先增加后减小</p>
<p>每次合并后词表可能出现3种变化：</p>
<ul>
<li><code>+1</code>，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词不是完全同时连续出现）</li>
<li><code>+0</code>，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（一个字词完全随着另一个字词的出现而紧跟着出现）</li>
<li><code>-1</code>，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现）</li>
</ul>
<h3 id="wordpiece">7.2.2 WordPiece</h3>
<p>WordPiece，从名字好理解，它是一种<strong>子词粒度的tokenize算法</strong>subword
tokenization
algorithm，很多著名的Transformers模型，比如BERT/DistilBERT/Electra都使用了它。</p>
<p>wordpiece算法可以看作是BPE的变种。不同的是，WordPiece基于概率生成新的subword而不是下一最高频字节对。WordPiece算法也是每次从词表中选出两个子词合并成新的子词。BPE选择频数最高的相邻子词合并，而WordPiece选择使得语言模型概率最大的相邻子词加入词表。即它每次合并的两个字符串A和B，应该具有最大的<span
class="math inline">\(\frac{P(A B)}{P(A)
P(B)}\)</span>值。合并AB之后，所有原来切成A+B两个tokens的就只保留AB一个token，整个训练集上最大似然变化量与<span
class="math inline">\(\frac{P(A B)}{P(A) P(B)}\)</span>成正比。</p>
<p><span class="math display">\[
\log P(S)=\sum_{i=1}^{n} \log P\left(t_{i}\right)
\]</span></p>
<p><span class="math display">\[
S=\left[t_{1}, t_{2}, t_{3}, \ldots, t_{n}\right]
\]</span></p>
<p>比如说 $ P(ed) <span class="math inline">\(的概率比\)</span>P(e) +
P(d)$
单独出现的概率更大，可能比他们具有最大的互信息值，也就是两子词在语言模型上具有较强的关联性。</p>
<p>那wordPiece和BPE的区别：</p>
<ul>
<li><strong>BPE</strong>： apple 当词表有appl 和
e的时候，apple优先编码为 appl和e（即使原始预料中 app 和 le
的可能性更大）</li>
<li><strong>wordPiece</strong>：根据原始语料， app和le的概率更大 </li>
</ul>
<h3 id="unigram">7.2.3 Unigram</h3>
<p>与BPE或者WordPiece不同，Unigram的算法思想是<strong>从一个巨大的词汇表出发</strong>，再<strong>逐渐删除trim
down其中的词汇</strong>，直到size满足预定义。</p>
<p>初始的词汇表可以<strong>采用所有预分词器分出来的词，再加上所有高频的子串</strong>。</p>
<p>每次从词汇表中删除词汇的<strong>原则是使预定义的损失最小</strong>。训练时，计算loss的公式为：</p>
<p><span class="math display">\[
Loss =-\sum_{i=1}^{N} \log \left(\sum_{x \in S\left(x_{i}\right)}
p(x)\right)
\]</span></p>
<p>假设训练文档中的所有词分别为<span class="math inline">\(x_{1} ;
x_{2}, \ldots,
x_{N}\)</span>，而<strong>每个词tokenize的方法</strong>是一个集合<span
class="math inline">\(S\left(x_{i}\right)\)</span></p>
<p>当一个词汇表确定时，每个词tokenize的方法集合<span
class="math inline">\(S\left(x_{i}\right)\)</span>就是确定的，而每种方法对应着一个概率<span
class="math inline">\(P(x)\)</span>.</p>
<p>如果从词汇表中删除部分词，则某些词的tokenize的种类集合就会变少，log(
*)中的求和项就会减少，从而增加整体loss。</p>
<p>Unigram算法每次<strong>会从词汇表中挑出使得loss增长最小的10%~20%的词汇</strong>来删除。</p>
<p>一般Unigram算法会与SentencePiece算法连用。</p>
<h3 id="sentencepiece">7.2.4 SentencePiece</h3>
<p>SentencePiece，顾名思义，它是<strong>把一个句子看作一个整体，再拆成片段</strong>，而没有保留天然的词语的概念。一般地，它<strong>把空格space也当作一种特殊字符来处理，再用BPE或者Unigram算法来构造词汇表</strong>。</p>
<p>比如，XLNetTokenizer就<strong>采用了_来代替空格</strong>，解码的时候会再用空格替换回来。</p>
<p>目前，Tokenizers库中，所有使用了SentencePiece的都是与Unigram算法联合使用的，比如ALBERT、XLNet、Marian和T5.</p>
<h1 id="长度外推问题篇">8 长度外推问题篇</h1>
<h2 id="长度外推问题">8.1 长度外推问题</h2>
<p>大模型的外推性问题是指<strong>大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题</strong>。在目前的大模型中，一般指的是超出预训练设置的上下文长度时，依旧保持良好推理效果的能力。</p>
<p>长度外推性=train short, test long</p>
<p><strong>train
short</strong>：1）受限于训练成本；2）大部分文本的长度不会特别长，训练时的max_length特别特别大其实意义不大（长尾）。</p>
<p><strong>test
long</strong>：这里long是指比训练时的max_length长，希望不用微调就能在长文本上也有不错的效果。</p>
<h2 id="长度外推问题的解决方法有哪些">8.2
长度外推问题的解决方法有哪些？</h2>
<h3 id="进制表示">（1）进制表示</h3>
<p>我们将整数n以一个三维向量[a,b,c]来输入，a,b,c分别是n的百位、十位、个位。这样，我们既缩小了数字的跨度，又没有缩小相邻数字的差距，代价了增加了输入的维度——刚好，神经网络擅长处理高维数据。</p>
<p>如果想要进一步缩小数字的跨度，我们还可以进一步缩小进制的基数，如使用8进制、6进制甚至2进制，代价是进一步增加输入的维度。</p>
<h3 id="直接外推">（2）直接外推</h3>
<p>简单来说，假如原来位置编码用三维向量表示，那外插就是直接增加一维。</p>
<p>可以提前预留多几维，训练阶段设为0，推理阶段直接改为其他数字，这就是外推（Extrapolation）。</p>
<p><img src="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240314113451952.png" srcset="/img/loading.gif" lazyload alt="长度外推问题的示意图" style="zoom:50%;" /></p>
<p>然而，训练阶段预留的维度一直是0，如果推理阶段改为其他数字，效果不见得会好，因为模型对没被训练过的情况不一定具有适应能力。也就是说，<strong>由于某些维度的训练数据不充分，所以直接进行外推通常会导致模型的性能严重下降</strong>。</p>
<h3 id="线性插值">（3）线性插值</h3>
<p>就是将2000以内压缩到1000以内，比如通过除以2，1749就变成了874.5，然后转为三维向量[8,7,4.5]输入到原来的模型中。从绝对数值来看，新的[7,4,9]实际上对应的是1498，是原本对应的2倍，映射方式不一致；从相对数值来看，原本相邻数字的差距为1，现在是0.5，最后一个维度更加“拥挤”。所以，做了内插修改后，通常都需要微调训练，以便模型重新适应拥挤的映射关系。</p>
<p><img src="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240314113638944.png" srcset="/img/loading.gif" lazyload alt="线性插值的表示" style="zoom:50%;" /></p>
<h3 id="进制转换">（4）进制转换</h3>
<p>有没有不用新增维度，又能保持相邻差距的方案呢？<strong>进制转换</strong>！三个数字的10进制编码可以表示0～999，如果是16进制呢？它最大可以表示163−1=4095&gt;1999。所以，只需要转到16进制，如1749变为[6,13,5]，那么三维向量就可以覆盖目标范围，代价是每个维度的数字从0～9变为0～15。</p>
<figure>
<img
src="https://gitee.com/lihaibineric/picgo/raw/master/pic/image-20240314113741425.png" srcset="/img/loading.gif" lazyload
alt="进制转换的示意图" />
<figcaption aria-hidden="true">进制转换的示意图</figcaption>
</figure>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="print-no-link">#深度学习</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>【深度学习】DeepL｜LLM基础知识</div>
      <div>https://lihaibineric.github.io/2024/03/05/dl_llm_basic/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Haibin Li</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>March 5, 2024</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Updated on</div>
          <div>March 24, 2024</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/03/08/dl_llm_model/" title="【大语言模型】基础模型概念">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">【大语言模型】基础模型概念</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/02/04/dl_rec_al/" title="【深度学习】推荐系统算法总结">
                        <span class="hidden-mobile">【深度学习】推荐系统算法总结</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
